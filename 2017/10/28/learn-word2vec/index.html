<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="xr6fZC8T0WK2mTcvk1n3Zus9z1ERpjekU-CN-SsZDCI">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"luckcul.net","root":"/","scheme":"Mist","version":"8.0.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>

  <meta name="description" content="word2vec是自然语言处理中非常重要的基础。最近，读了一下相关论文，学习了相关的模型细节，并使用tensorflow实现了网络结构，对中文维基百科语料库进行了训练。相关论文及实现参考见文末。">
<meta property="og:type" content="article">
<meta property="og:title" content="Word2Vec学习及基于tensorflow的实现">
<meta property="og:url" content="http://luckcul.net/2017/10/28/learn-word2vec/index.html">
<meta property="og:site_name" content="luckcul&#39;s blog">
<meta property="og:description" content="word2vec是自然语言处理中非常重要的基础。最近，读了一下相关论文，学习了相关的模型细节，并使用tensorflow实现了网络结构，对中文维基百科语料库进行了训练。相关论文及实现参考见文末。">
<meta property="og:locale">
<meta property="og:image" content="http://res.cloudinary.com/dyhtzpcxp/image/upload/c_scale,w_500/v1509196310/cbow_lbiet9.png">
<meta property="og:image" content="http://res.cloudinary.com/dyhtzpcxp/image/upload/c_scale,w_500/v1509197566/igSuE_au4ip3.png">
<meta property="og:image" content="http://res.cloudinary.com/dyhtzpcxp/image/upload/v1509346849/2017-10-28_23-02-55%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE_izqrvh.png">
<meta property="og:image" content="http://res.cloudinary.com/dyhtzpcxp/image/upload/v1509449657/blog/show_.png">
<meta property="article:published_time" content="2017-10-28T09:05:19.000Z">
<meta property="article:modified_time" content="2018-02-06T16:00:00.000Z">
<meta property="article:author" content="luckcul">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="deep learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://res.cloudinary.com/dyhtzpcxp/image/upload/c_scale,w_500/v1509196310/cbow_lbiet9.png">


<link rel="canonical" href="http://luckcul.net/2017/10/28/learn-word2vec/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Word2Vec学习及基于tensorflow的实现 | luckcul's blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-70247022-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-70247022-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?34453bc9c2eaf52085c082e08202c30a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">luckcul's blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">越努力，越幸运</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">1.</span> <span class="nav-text">关于词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#One-hot-Representation"><span class="nav-number">1.1.</span> <span class="nav-text">One-hot Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-Representation"><span class="nav-number">1.2.</span> <span class="nav-text">Distributed Representation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BB%86%E8%8A%82"><span class="nav-number">2.</span> <span class="nav-text">训练模型与细节</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CBOW-%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">CBOW 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Skip-gram-%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">Skip-gram 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">2.3.</span> <span class="nav-text">评价指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-number">2.4.</span> <span class="nav-text">优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hierarchical-Softmax"><span class="nav-number">2.4.1.</span> <span class="nav-text">Hierarchical Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Negative-Sampling"><span class="nav-number">2.4.2.</span> <span class="nav-text">Negative Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Subsampling"><span class="nav-number">2.4.3.</span> <span class="nav-text">Subsampling</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB"><span class="nav-number">3.</span> <span class="nav-text">源码阅读</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%8F%8A%E5%AF%B9%E6%AF%94"><span class="nav-number">4.</span> <span class="nav-text">参数选择及对比</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%B4%E5%9F%BA%E4%B8%AD%E6%96%87%E8%AF%AD%E6%96%99%E5%BA%93%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">5.</span> <span class="nav-text">基于维基中文语料库的训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="luckcul"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">luckcul</p>
  <div class="site-description" itemprop="description">Life is a Marathon!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/luckcul" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;luckcul" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/tian-yan-fei" title="ZhiHu → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;tian-yan-fei" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>ZhiHu</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://luckcul.net/2017/10/28/learn-word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="luckcul">
      <meta itemprop="description" content="Life is a Marathon!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="luckcul's blog">
    </span>

    
    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Word2Vec学习及基于tensorflow的实现
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-10-28 17:05:19" itemprop="dateCreated datePublished" datetime="2017-10-28T17:05:19+08:00">2017-10-28</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2018-02-07 00:00:00" itemprop="dateModified" datetime="2018-02-07T00:00:00+08:00">2018-02-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>word2vec是自然语言处理中非常重要的基础。最近，读了一下相关论文，学习了相关的模型细节，并使用tensorflow实现了网络结构，对中文维基百科语料库进行了训练。相关论文及实现参考见文末。</p>
<a id="more"></a>

<h1 id="关于词向量"><a href="#关于词向量" class="headerlink" title="关于词向量"></a>关于词向量</h1><p>在自然语言处理中，第一步就是要将自然语言数字化。所以，词向量的表示方式就很重要。最常见的有One-hot Representation和 Distributed Representation。</p>
<h2 id="One-hot-Representation"><a href="#One-hot-Representation" class="headerlink" title="One-hot Representation"></a>One-hot Representation</h2><p>这是一种很直接的表示方式，一个词向量中只有一个维度的值是1，其它的绝大多数是0。例如“dog”这个词的one-hot表示为<code>[0, 0, 1, 0, ......]</code>。 </p>
<p>这种表示方法得到的向量是非常稀疏的，维度非常大，会造成维度灾难。这种表示方法有一个很大的问题，如果两个词之间存在着关联，如果以one-hot方式表示的话，这部分关联信息就会丧失。模型几乎不能利用从“dog”学来的东西来处理“cat”，而distributed的表示方法能很好的解决这个问题。</p>
<h2 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h2><p>这种表示方法下，词向量的维度比较低，一般不会超过几百维。它的主要思想是，（句法和语义上）相近的词有相近的向量表示。这个相近的向量表示，可以用两个词向量之间的cosine夹角来度量。</p>
<p>这种表示方法也被成为word embedding。关于”distributed”的一种解释是，不像one-hot那样只在一个维度上有非零值。目前主要有两种训练模型：CBOW (Continuous Bag of Words) 和 Skip-gram model，两种模型比较相近。</p>
<h1 id="训练模型与细节"><a href="#训练模型与细节" class="headerlink" title="训练模型与细节"></a>训练模型与细节</h1><h2 id="CBOW-模型"><a href="#CBOW-模型" class="headerlink" title="CBOW 模型"></a>CBOW 模型</h2><p><img src="http://res.cloudinary.com/dyhtzpcxp/image/upload/c_scale,w_500/v1509196310/cbow_lbiet9.png"></p>
<p>模型的输入为某个词$w_t$前后的某些词，输出为要预测的$w_t$的输出y。假如窗口大小为2，则输入为$w _{t-1}, w _{t+1}$ ，这些输入是one-hot向量。每个词先通过矩阵W，映射到各自对应的向量；然后把这些各自得到的向量相加；最后通过一个softmax层，获得相应的概率。<strong>其实，这个矩阵W，就是我们最终要得到的各个词的词向量的表示。</strong></p>
<h2 id="Skip-gram-模型"><a href="#Skip-gram-模型" class="headerlink" title="Skip-gram 模型"></a>Skip-gram 模型</h2><p><img src="http://res.cloudinary.com/dyhtzpcxp/image/upload/c_scale,w_500/v1509197566/igSuE_au4ip3.png"></p>
<p>和上面的模型非常像，输入为某一个词$w_t$ ，输出为这个词前后词的概率。</p>
<p>如果一份文档由T个词组成，$w_1, w_2, …, w_T$ ，那么skip-gram的目标函数就是最大化平均对数概率：<br>$$<br>\frac{1}{T} \sum_{t=1}^{T}\sum_{-c\leq j \leq c} \log p(w_{t+j } | w_t)<br>$$<br>其中，c表示中心词$w_t$前后各c个词。如果直接以softmax方式计算$p(w_O | w_I)$<br>$$<br>p(w_O|w_I) = \frac{\exp({v’<em>{w_O}}^\top v_{w_I})}{\sum_{w=1}^{W} \exp({v’</em>{w}}^\top v_{w_I})}<br>$$<br>其中，W是总词汇量，$v_{w}^{‘}$和$v_w$ 分别是词w的输出和输入的向量表示，即分别是上图中两个矩阵$W_{V \times N}, W_{N \times V}^{‘}$ 中的一行和一列。</p>
<p>然而，这样计算是不可行的，因为W实在太大了。</p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>论文中是采用analogical reasoning task来进行评价学到的词向量的好坏的。所谓的analogical reasoning task就是，比如给定<code>“Germany” : “Berlin” :: “France” : ?</code>。找到这样的词x，vec(x)和vec(“Berlin”) - vec(“Germany”) + vec(“France”)在cosine距离上最相近。这种推理任务分两类，(1)syntactic analogy和(2)semantic analogy。</p>
<p>然而，感觉这种评价指标有点片面，而且对于具体的问题可能这项指标的作用不大。可以根据具体的任务相关来评价它的好坏。</p>
<p>能想到的其他的评价方式，比如聚类看分布，通过cosine距离找近义词等。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>上面已经指出，计算softmax的复杂度太高，而通过Hierarchical Softmax，可以将每次的复杂度O(W)降到O(log(W))。</p>
<p>具体是，最后的输出层是一个二叉树，Huffman树。Huffman树是根据Huffman编码来进行得到的，这儿就不具体讲Huffman编码的算法了。最后的到的Huffman树，每个词代表的节点是叶子节点，而且词频越高的节点，深度越低。这样正向传播的时候，每次选择左子树，或者右子树，直到到达目标叶子节点，得到目标概率值。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>这也是对softmax层的优化，negative sampling的效率和效果都比Hierarchical Softmax要好些。Hierarchical Softmax是准确的计算概率，而这种方法是一种近似计算。对出目标词外的其他词进行负采样，可以降低其他单词的权重。</p>
<p>重新定义了上面的$p(w_O | w_I)$ ：<br>$$<br>p(w_O | w_I) = \log \sigma({v’<em>{w_O}}^\top v_{w_I} )+ \sum_{i=1}^{k}\mathbb{E}</em>{w_i \sim P_n(w)}[\log \sigma(-{v’_{w_i}}^\top v_{w_I})]<br>$$<br>其中，$ P_n(w)$ 是词w周围的噪声分布，具体的选取是根据词频越高，被选取的几率越大。k为除词w之外，其他要取的数量。论文中建议对于大的训练集，取2-5，小的训练集，取5-20。</p>
<h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>这是为了解决一些高频词，如”in”,”the”等。它们提供的信息非常少，即使在上百万的样本上进行训练，这些词在对应的词向量在训练时也不会发生显著变化；而且，这些高频词的出现对于正常词的训练帮助也很小。为了解决高频词和低频次之间的不平衡，每个词有一定概率被丢弃：<br>$$<br>P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}<br>$$<br>其中，f(wi)是词wi的出现频率,t是一个选定的阈值，通常为1e-5。</p>
<p>在实际中，表现的很好，加速了学习过程(对比试验中减少了几乎一半的时间)，甚至低频词的向量的更精确。</p>
<h1 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h1><p>阅读了<a target="_blank" rel="noopener" href="https://code.google.com/archive/p/word2vec/">word2vec源码</a>， 进一步了解了其中的细节。</p>
<ul>
<li>因为对sigmoid的精度要求不是很高，而且要重复用到。所以sigmoid 进行打表。</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</span><br><span class="line">  expTable[i] = <span class="built_in">exp</span>((i / (real)EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP); <span class="comment">// Precompute the exp() table</span></span><br><span class="line">  expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>);                   <span class="comment">// Precompute f(x) = x / (x + 1)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>网络初始化，<code>InitNet()</code> 开辟syn0 input vectors, 对于hs or negative 开辟syn1，并创建huffman tree,并写入到<code>vocab[].point and code</code>。output vector <code>syn1</code> 初始化为0,input vector <code>syn0</code>初始化为<code>[-0.5/layer_size, 0.5/layer_size]</code></li>
<li>对于Negative Sampling， <code>InitUnigramTable()</code> 是初始化table数组。按照采样概率，在这个1e8大小的table上，每个单词的出现的次数=被采样的概率*1e8。使用的是unigram distribution, 3/4是经验值。</li>
</ul>
<p>$$<br>P(w_i)=\frac{f(w_i)^{3/4}}{\sum_{j=0}^{n}f(w_j)^{3/4}}<br>$$</p>
<ul>
<li>对于下采样，代码和论文是不一样的，代码是这么写的。</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (sample &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  real ran = (<span class="built_in">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class="number">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class="line">  next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>)<span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">  <span class="keyword">if</span> (ran &lt; (next_random &amp; <span class="number">0xFFFF</span>) / (real)<span class="number">65536</span>) <span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据代码，一个词wi被保留的概率就是：<br>$$<br>P(w_i) = \sqrt {\frac{t}{f(w_i)}} + \frac{t}{f(w_i)}<br>$$</p>
<ul>
<li>alpha 学习速率的变化。初始0.025 <code>starting_alpha = alpha = 0.025</code>  <code>alpha</code>更新 :</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alpha = starting_alpha * (<span class="number">1</span> - word_count_actual / (real)(iter * train_words + <span class="number">1</span>));</span><br><span class="line"><span class="keyword">if</span> (alpha &lt; starting_alpha * <span class="number">0.0001</span>) alpha = starting_alpha * <span class="number">0.0001</span></span><br></pre></td></tr></table></figure>
<ul>
<li>并不是在<code>windows</code>里面的所有词来做训练， 而是random b = [0,windows)。然后<code>for (a = b; a &lt; window * 2 + 1 - b; a++)</code>。 </li>
<li><code>word2vec.c</code> 还提供了了通过k-mean聚成预先设定好的classes个类，对每个词聚类。</li>
</ul>
<h1 id="参数选择及对比"><a href="#参数选择及对比" class="headerlink" title="参数选择及对比"></a>参数选择及对比</h1><p>训练时主要的参数有选择训练模型cbow或者sg，词向量大小size，上下文窗口window，下采样的阈值sample，选择hierarchical softmax 或者负采样计算softmax，负采样的数量negative，迭代数据次数iter，词表限制的词频阈值min-count等。</p>
<p>首先对于两个训练模型训练速度上进行了对比。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15</span><br><span class="line">time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 0  -hs 1 -sample 1e-4 -threads 20 -binary 1 -iter 15</span><br><span class="line">time ./word2vec -train text8 -output vectors.bin -cbow 0 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15</span><br><span class="line">time ./word2vec -train text8 -output vectors.bin -cbow 0 -size 200 -window 8 -negative 0 -hs 1  -sample 1e-4 -threads 20 -binary 1 -iter 15</span><br></pre></td></tr></table></figure>
<p>训练时间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Starting training using file text8</span><br><span class="line">Vocab size: 71291</span><br><span class="line">Words in train file: 16718843</span><br><span class="line">Alpha: 0.000005  Progress: 100.10%  Words&#x2F;thread&#x2F;sec: 89.08k  </span><br><span class="line">real	3m4.607s</span><br><span class="line">user	47m0.884s</span><br><span class="line">sys	0m2.292s</span><br><span class="line">Starting training using file text8</span><br><span class="line">Vocab size: 71291</span><br><span class="line">Words in train file: 16718843</span><br><span class="line">Alpha: 0.000005  Progress: 100.11%  Words&#x2F;thread&#x2F;sec: 172.98k  </span><br><span class="line">real	1m37.869s</span><br><span class="line">user	24m14.380s</span><br><span class="line">sys	0m1.440s</span><br><span class="line">Starting training using file text8</span><br><span class="line">Vocab size: 71291</span><br><span class="line">Words in train file: 16718843</span><br><span class="line">Alpha: 0.000002  Progress: 100.11%  Words&#x2F;thread&#x2F;sec: 11.15k  </span><br><span class="line">real	23m43.240s</span><br><span class="line">user	375m19.832s</span><br><span class="line">sys	0m3.400s</span><br><span class="line">Starting training using file text8</span><br><span class="line">Vocab size: 71291</span><br><span class="line">Words in train file: 16718843</span><br><span class="line">Alpha: 0.000002  Progress: 100.11%  Words&#x2F;thread&#x2F;sec: 22.72k  </span><br><span class="line">real	11m51.509s</span><br><span class="line">user	184m14.460s</span><br><span class="line">sys	0m1.452s</span><br></pre></td></tr></table></figure>
<p>可以看出使用sg模型远比使用cbow要慢。negative=25的时候，比使用hs要慢，对比论文中的实验，当neg=5的时候和hs效率相当。</p>
<p>sg模型获得的低频词的词向量质量比cbow好一些。hs对于低频词表现更好，负采样对于高频词表现更好。关于其他参数的比较与较好的经验设置，可以参考<a target="_blank" rel="noopener" href="http://licstar.net/archives/620">《How to Generate a Good Word Embedding?》</a>。 </p>
<blockquote>
<p>首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。</p>
</blockquote>
<p>其中也说，<strong>语料对词向量的影响比模型的影响要重要</strong>。做具体 NLP 任务（用作特征、用作神经网络初始化）的时候，50 维之后效果提升就比较少了。</p>
<h1 id="基于维基中文语料库的训练"><a href="#基于维基中文语料库的训练" class="headerlink" title="基于维基中文语料库的训练"></a>基于维基中文语料库的训练</h1><p>采用的数据是<a target="_blank" rel="noopener" href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2">中文维基百科</a>的数据，进行了数据抽取，繁体转简体，分词。</p>
<p>基于tensorflow来进行训练，在计算loss时，用了<code>tf.nn.nce_loss</code>来进行计算，它的输入是隐藏层到输出层的权重矩阵和偏差，隐藏层矩阵，真实值label，词典大小和negative sampling中采样大小k值。</p>
<p>实际的训练效果，示例如下：</p>
<p><img src="http://res.cloudinary.com/dyhtzpcxp/image/upload/v1509346849/2017-10-28_23-02-55%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE_izqrvh.png"></p>
<p>利用t-SNE对出现频度最高的500个词的词向量进行降维，下面是降维到2维的结果。</p>
<p><img src="http://res.cloudinary.com/dyhtzpcxp/image/upload/v1509449657/blog/show_.png"></p>
<p>从图中可以看出，一些国家名称聚在了一起；”国王”，”皇帝”，”主席”，”总统”聚在了一起；”作品”，”故事”，”小说”等比较相近。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1].Mikolov, T., Sutskever, I., Chen, K., Corrado, G. &amp; Dean, J. Distributed representations of words and phrases and their compositionality. (NIPS 2013)</p>
<p>[2].<a target="_blank" rel="noopener" href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a></p>
<p>[3]. <a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/word2vec">TensorFlow: Vector Representations of Words</a></p>
<p>[4].<a target="_blank" rel="noopener" href="https://dumps.wikimedia.org/">Wikimedia Downloads</a></p>
<p>[5].<a target="_blank" rel="noopener" href="http://licstar.net/archives/620">《How to Generate a Good Word Embedding?》</a></p>
<p>[6]. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/09/04/leetcode-188/" rel="prev" title="leetcode 188. Best Time to Buy and Sell Stock IV">
                  <i class="fa fa-chevron-left"></i> leetcode 188. Best Time to Buy and Sell Stock IV
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/02/04/ccf-bdci-ai-judge/" rel="next" title="2017 CCF-BDCI 让AI当法官 第一名解决方案">
                  2017 CCF-BDCI 让AI当法官 第一名解决方案 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
  
  
  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">luckcul</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script>
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','UfzrtLcd_2sN6-teqqRL','2.0.0');
</script>













  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.0/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
